<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="keywords" content="">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover">
    <link rel="shortcut icon" type="image/png" href="favicon.png">
    
	<link rel="stylesheet" type="text/css" href="./css/bootstrap.min.css?1330">
	<link rel="stylesheet" type="text/css" href="style.css?6406">
	<link rel="stylesheet" type="text/css" href="./css/font-awesome.min.css">
	<link rel="stylesheet" type="text/css" href="./css/linecons.min.css">
	<link href='https://fonts.googleapis.com/css?family=PT+Serif+Caption&display=swap&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
	<link href='https://fonts.googleapis.com/css?family=Vollkorn&display=swap&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
    <title>Home</title>


    
<!-- Analytics -->
 
<!-- Analytics END -->
    
</head>
<body>
<!-- Main container -->
<div class="page-container">
    
<!-- bloc-0 -->
<div class="bloc l-bloc" id="bloc-0">
	<div class="container bloc-sm">
		<div class="row">
			<div class="col">
				<nav class="navbar navbar-light row navbar-expand-md" role="navigation">
					<a class="navbar-brand" href="index.html">Lingyun Yu</a>
					<button id="nav-toggle" type="button" class="ml-auto ui-navbar-toggler navbar-toggler border-0 p-0" data-toggle="collapse" data-target=".navbar-576" aria-expanded="false" aria-label="Toggle navigation">
						<span class="navbar-toggler-icon"></span>
					</button>
					<div class="collapse navbar-collapse navbar-576">
						<ul class="site-navigation nav navbar-nav ml-auto">
							<li class="nav-item">
								<a href="index.html" class="nav-link">Home</a>
							</li>
							<li class="nav-item">
							</li>
							<li class="nav-item">
							</li>
							<li class="nav-item">
								<a href="research.html" class="a-btn nav-link">Research</a>
							</li>
						</ul>
					</div>
				</nav>
			</div>
		</div>
	</div>
</div>
<!-- bloc-0 END -->

<!-- ScrollToTop Button -->
<a class="bloc-button btn btn-d scrollToTop" onclick="scrollToTarget('1',this)"><span class="fa fa-chevron-up"></span></a>
<!-- ScrollToTop Button END-->


<!-- bloc-1 -->
<div class="bloc l-bloc" id="bloc-1">
	<div class="container bloc-lg">
		<div class="row bgc-1570">
			<div class="col-md-6">
				<img src="img/lazyload-ph.png" data-src="img/me3.jpg" class="img-fluid mx-auto d-block lazyload" alt="me3" />
			</div>
			<div class="col-md-6 align-self-end">
				<h2 class="mg-md h2-style">
					Lingyun Yu
				</h2><img src="img/lazyload-ph.png" data-src="img/name.png" class="img-fluid img-style float-lg-none mg-md lazyload" alt="name" />
				<p class="btn-resize-mode p-1-style mg-clear">
					<strong>Doctor</strong><br>University of Science and Technology of China<br><br><a href="./pdf/CV_LingyunYu_Eng.pdf" target="_blank">[CV(English)]</a>&nbsp;<a href="./pdf/CV_LingyunYu_Chn.pdf" target="_blank">[CV(Chinese)]</a>&nbsp;<a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=M1fL0BwAAAAJ" target="_blank">[Google Scholar]</a><br><br><br><strong>Phone:</strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; +86-13966763354<br><strong>Email:</strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;yuly@mail.ustc.edu.cn<br><strong>HomePage:</strong> &nbsp; &nbsp; http://home.ustc.edu.cn/~yuly/<br><strong>GitHub:</strong> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;https://github.com/xiaoyun4<br><strong>Address:</strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;West Campus USTC, No.443, Huangshan Road, Hefei, Anhui, 230027, P. R. China<br>
				</p>
			</div>
		</div>
		<div class="row">
			<div class="col">
				<p>
					<br>
				</p>
			</div>
		</div>
		<div class="row">
			<div class="col">
				<p class="p-4-style btn-resize-mode">
					<strong>&nbsp; &nbsp; &nbsp; I will finish my doctor career in the year 2020. And now I am hunting for jobs about deep learning and multimedia processing. My job intention is post of Computer Vision or Speech Visualization.</strong><br><br>&nbsp; &nbsp; &nbsp; My research interests span <strong>Talking face generation</strong>, <strong>Multi-modal learning</strong>, Articulatory movements-driven <strong>3D Talking Head</strong>, <strong>Human-Computer Interaction</strong> and <strong>Video synthesis</strong>. The noteworthy research project of mine is to g<strong>enerate realistic talking heads</strong> synchronized with multi-modal input (audio or text). I am skilled at python and matlab coding, and experienced of Linux system.<br>
				</p>
				<h3 class="mg-md">
					<span class="fa fa-bullhorn"></span>&nbsp; News!
				</h3>
				<p class="btn-resize-mode p-bloc-1-style">
					<strong>&nbsp;<strong>2020/02/10</strong></strong> &nbsp; One Journal paper accepted by IEEE Transactions on on Circuits and Systems for Video Technology (Impact Factor 4.046)!<br>
					<strong>&nbsp;<strong>2019/11/10</strong></strong> &nbsp; One regular paper accepted by AAAI 2020!<br>
					<strong>&nbsp;<strong>2019/09/02</strong></strong> &nbsp; One Journal paper accepted by IEEE Transactions on Multimedia (Impact Factor 5.452)!<br>
					<strong>&nbsp;<strong>2019/08/09</strong></strong> &nbsp; One regular paper accepted by ICDM 2019 (9.08% acceptance rate)!<br>
					<strong>&nbsp;<strong>2019/06/24</strong></strong> &nbsp; First place in ACM MM2019 “AI Meets Beauty” Challenge!<br>
				</p>
				<h3 class="mg-md">
					<span class="fa fa-mortar-board"></span>&nbsp; Education
				</h3>
				<div class="blockquote">
					<p class="btn-resize-mode p-5-style">
						<strong>2020/01-2020/04<br></strong>Visiting Student<br> the University of Sydney, Australia (USYD)<br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-5-style">
						<strong>2015/09-Present<br></strong>Doctor of Control Science and Engineering<br>University of Science and Technology of China (USTC)<br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-6-style">
						<strong>2011/09-2015/06<br></strong>Bachelor of&nbsp;Electrical Engineering and Automation<br>China University of Mining and Technology (CUMT)<br>
					</p>
				</div>
				<h3 class="mg-md">
					<span class="fa fa-address-card-o"></span>&nbsp;&nbsp;Internship<br>
				</h3>
				<div class="blockquote">
					<p class="btn-resize-mode p-6-bloc-1-style p-7-bloc-1-style">
						2017/07-2019/01 &nbsp;iFlytek&nbsp;Co. Ltd.<br>&nbsp; &nbsp; <i>Core Technology Researcher</i><br>● &nbsp;Given an arbitrary speech clip or text information as input, we aim to generate a talking face video with accurate lip synchronization.<br>
					</p>
				</div>
				<h3 class="mg-md">
					<span class="li_search"></span>&nbsp; Research<br>
				</h3>
				<div class="blockquote">
					<p class="btn-resize-mode p-8-style">
						<strong>10/2017-Present &nbsp;Talking Face Generation from Text or Audio input<br></strong><i>&nbsp; &nbsp; Independent Completor<br></i>&nbsp; &nbsp; ● &nbsp;Decompose the talking face generation task into two steps: mouth landmarks prediction and video synthesis.<br>&nbsp; &nbsp; ● &nbsp;<strong>The time-delayed LSTM</strong> is adopted to predict accurate mouth landmarks.<br>&nbsp; &nbsp; ● &nbsp;A network named Face2Vid is proposed to generate video frames conditioned on the predicted mouth landmarks. In ßFace2Vid, the <strong>optical flow</strong> is employed to model the temporal dependency between frames, meanwhile, a <strong>self-attention mechanism</strong> is introduced to model the spatial dependency across image regions.<br><i>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; One Journal paper accepted by IEEE TCSVT.<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; One conference paper accepted by ICDM 2019.</i><br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-8-style">
						<strong>05/2016-09/2017 Articulatory Synchronicity from Text and Audio Inputs<br></strong><i>&nbsp; &nbsp; Independent Completor<br></i>&nbsp; &nbsp; ● &nbsp;Propose a new network architecture for articulatory movement prediction with both text and audio inputs, called BLTRCNN.<br>&nbsp; &nbsp; ● &nbsp;The bottleneck network is adopted to extract the compact bottleneck features as the complementary linguistic features for better performance.<br>&nbsp; &nbsp; ● &nbsp;Combining CNN, LSTM and skip connection can not only acquire local higher-level features but also learn long-range of context information.<br>&nbsp; &nbsp; ● &nbsp;Combining acoustic features and linguistic features as inputs is able to contain more information to boost the performance.<br><i>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; One Journal paper accepted by IEEE TMM.<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Two conference papers accepted by MMM2019 and VCIP2018.<br></i><br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-8-style">
						<strong>09/2017-11/2017 The personalized co-articulation rule statistics<br></strong><i>&nbsp; &nbsp; Independent Completor<br></i>&nbsp; &nbsp; ● &nbsp;We aim at studying the personalized co-articulation rule among neighboring phonemes by the representation of the mouth shape with both text and audio information as inputs.<br>&nbsp; &nbsp; ● &nbsp;A time-delayed LSTM is used to model the mapping from the linguistic and acoustic features to the mouth landmarks.<br>&nbsp; &nbsp; ● &nbsp;By statistical knowledge, we calculate and study the co-articulation rule among neighboring phonemes<br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-8-style">
						<strong>02/2015-02/2016 Articulatory movements-driven 3D pronunciation system<br></strong><i>&nbsp; &nbsp; Independent Completor<br></i>&nbsp; &nbsp; ● &nbsp;Propose an emotional text-driven 3D visual pronunciation system for Mandarin Chinese by generating the emotional articulatory movement trajectory.<br>&nbsp; &nbsp; ● &nbsp;The articulatory movements are predicted by HMM.<br>&nbsp; &nbsp; ● &nbsp;Analyze and summarize the variation of the articulatory movements under different emotions.<br><i>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; One Journal paper accepted by Multimedia Tools and Applications.<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Conference papers accepted by CCPR2016.</i><br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-8-style">
						<strong>04/2019-07/2019 &nbsp;ACMMultimedia “AI Meets Beauty” Challenge<br></strong><i>&nbsp; &nbsp; Project Participant<br></i>&nbsp; &nbsp; ● &nbsp;Propose a novel Generalized-attention Regional Maximal Activation of Convolutions (GR-MAC) descriptor to boosts retrieval performance.<br>&nbsp; &nbsp; ● &nbsp;Attention mechanism is introduced to assign larger weights for target regions.<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <i>Awarded the 1st place in the Grand Challenge of AI Meets Beauty.<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Conference papers accepted by ACM MM 2019 workshop.</i><br>
					</p>
				</div>
				<h3 class="mg-md">
					<span class="fa fa-newspaper-o"></span>&nbsp;&nbsp;Publications<br>
				</h3>
				<div class="blockquote">
					<p class="btn-resize-mode p-15-style p-16-bloc-1-style">
						Multimodal Inputs Driven Talking Face Generation With Spatial-Temporal Dependency<br><strong>Lingyun Yu</strong>, Jun Yu, Mengyan Li and Qiang Ling.<br>IEEE Transactions on Circuits and Systems for Video Technology&nbsp;
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-7-style">
						Mining Audio, Text and Visual information for Talking face generation<br><strong>Lingyun Yu</strong>, Jun Yu<br>IEEE International Conference on Data Mining (ICDM) 2019 (<strong>Regular paper: 9.08% acceptance rate</strong>)<br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-7-style">
						BLTRCNN Based 3D Articulatory Movement Prediction: Learning Articulatory Synchronicity From Both Text and Audio Inputs<br><strong>Lingyun Yu</strong>, Jun Yu, and Qiang Ling.<br>IEEE Transactions on Multimedia<br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-7-style">
						A realistic 3D articulatory animation system for emotional visual pronunciation<br><strong>Lingyun Yu</strong>, Jun Yu<br>Multimedia Tools and Applications<br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-26029-style">
						Deep Neural Network Based 3D Articulatory Movement Prediction Using Both Text and Audio Inputs<br><strong>Lingyun Yu</strong>, Jun Yu, Ling Qiang<br>International Conference on Multimedia Modeling
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-12-bloc-1-style p-14-style">
						Synthesizing 3D Acoustic-Articulatory Mapping Trajectories: Predicting Articulatory Movements by Long-Term Recurrent Convolutional Neural Network. <br><strong>Lingyun Yu</strong>, Jun Yu, Ling Qiang&nbsp;<br>IEEE International Conference on Visual Communications and Image Processing (VCIP)
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-9-bloc-1-style">
						An Emotional Text-Driven 3D Visual Pronunciation System for Mandarin Chinese<br><strong>Lingyun Yu</strong>, Changwei Luo, Jun Yu<br>Chinese Conference on Pattern Recognition (CCPR)<br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-10-style">
						Synthesizing Photo-Realistic 3D Talking Head: Learning Lip Synchronicity and Emotion from Audio and Video<br>Jun Yun, <strong>Lingyun Yu</strong><br>IEEE International Conference on Image Processing (ICIP)<br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-11-bloc-1-style">
						Beauty Product Retrieval Based on Regional Maximum Activation of Convolutions with Generalized Attention.<br>Jun Yun, <strong>Lingyun Yu</strong>, et al.<br>ACM MM 2019 workshop<br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-16-style">
						Bidirectional Attention-Recognition Model for Fine-grained Object Classification<br>Chuanbin Liu, <strong>Lingyun Yu</strong>, et al.<br>IEEE Transactions on Multimedia<br>
					</p>
				</div>
				<div class="blockquote">
					<p class="btn-resize-mode p-16-style">
						Filtration and Distillation: Enhancing Region Attention for Fine-Grained Visual Categorization<br>Chuanbin Liu, Hongtao Xie, Zhengyun Zha,Lingfeng Ma, <strong>Lingyun Yu，</strong>Yongdong Zhang.<br>The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)<br>
					</p>
				</div>
				<h3 class="mg-md">
					<span class="fa fa-certificate"></span>&nbsp; Awards<br>
				</h3>
				<p class="p-9-style ">
					<strong>2019/12</strong>&nbsp;Travel Award, 2019 IEEE International Conference on Data Mining
				<p class="p-9-style ">
					<strong>2019/10</strong>&nbsp;Suzhou Industrial Park Scholarship, USTC
				<p class="p-9-style ">
					<strong>2019/07</strong>&nbsp;1st place, ACMMultimedia "AI Meets Beauty" challenge
				</p>
				<p class="p-9-style">
					<strong>2017/11</strong> Travel Award, 2017 IEEE International Conference on Visual Communications and Image Processing
				</p>
				<p class="p-style btn-resize-mode">
					<strong>2017/09</strong> National Scholarships, USTC<br>
				</p>
				<p class="p-11-style">
					<strong>2015/05</strong>&nbsp;Excellent graduates, CUMT
				</p>
				<p class="p-12-style">
					<strong>2014/10</strong>&nbsp;Exam-Free Postgraduate to USTC
				</p>
				<h3 class="mg-md">
					<span class="li_banknote"></span>&nbsp; Fundation<br>
				</h3>
			</div>
		</div>
	</div>
</div>
<!-- bloc-1 END -->

</div>
<!-- Main container END -->
    

<script src="./js/jquery-3.3.1.min.js?1820"></script>
<script src="./js/bootstrap.bundle.min.js?2766"></script>
<script src="./js/blocs.min.js?683"></script>
<script src="./js/lazysizes.min.js" defer></script>
<!-- Additional JS END -->



<!-- Preloader -->
<div id="page-loading-blocs-notifaction" class="page-preloader"></div>
<!-- Preloader END -->

</body>
</html>
